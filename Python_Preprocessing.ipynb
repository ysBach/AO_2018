{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Python Preprocessing\n",
    "\n",
    "In this note, I will show basic method to do the preprocessing. The idea is simple:\n",
    "\n",
    "1. Change file names (see the section 1 below)\n",
    "2. Subtract bias and/or dark from object and flat images\n",
    "3. Do flat fielding to the object image.\n",
    "\n",
    "Details of the last two steps:\n",
    "\n",
    "1. Dark of exposure time < 1 s is regarded as bias\n",
    "2. Flat is bias subtracted, and then the Poisson error is calculated.\n",
    "3. Do dark subtraction and flat fielding for only the images which has proper ``OBJECT`` keyword in header.\n",
    "\n",
    "I will use 2018-04-12 data of TRIPOL obtained by the \"comet\" group and by the TAs. The data is accessible from our NAS server:\n",
    "* https://147.46.40.80:5003\n",
    "* ID = tripol PW = (same as astro wifi)\n",
    "\n",
    "\n",
    "First, the following long code contains definitions of some useful functions. I will use them throughout this lecture note. You may use these functions freely for your group project works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import itemfreq\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy import units as u\n",
    "from astropy.table import Table, Column\n",
    "from astropy.nddata import CCDData\n",
    "from astropy.modeling.functional_models import Gaussian1D\n",
    "from astropy.modeling.fitting import LevMarLSQFitter\n",
    "\n",
    "from ccdproc import combine\n",
    "from ccdproc import sigma_func as ccdproc_mad2sigma_func\n",
    "\n",
    "\n",
    "def mkdir(fpath, mode=0o777, exist_ok=True):\n",
    "    ''' Convenience function for Path.mkdir()\n",
    "    '''\n",
    "    fpath = Path(fpath)\n",
    "    Path.mkdir(fpath, mode=mode, exist_ok=exist_ok)\n",
    "    \n",
    "    \n",
    "def fitsrenamer(fpath, rename_by=[\"OBJECT\"], delimiter='_', add_header=None,\n",
    "    mkdir_by=None, verbose=True):\n",
    "    ''' Renames a FITS file by ``rename_by`` with delimiter.\n",
    "    Parameters\n",
    "    ----------\n",
    "    fpath: path-like\n",
    "        The path to the target FITS file.\n",
    "    rename_by: list of str, optional\n",
    "        The keywords of the FITS header to rename by.\n",
    "    delimiter: str, optional\n",
    "        The delimiter for the renaming.\n",
    "    add_header: header or dict\n",
    "        The header keyword, value (and comment) to add after the renaming.\n",
    "    mkdir_by: list of str, optional\n",
    "        The keys which will be used to make subdirectories to classify files.\n",
    "        If given, subdirectories will be made with the header value of the keys.\n",
    "    '''\n",
    "\n",
    "    # Load fits file\n",
    "    hdul = fits.open(fpath)\n",
    "    hdr = hdul[0].header\n",
    "\n",
    "    # add keyword\n",
    "    if add_header is not None:\n",
    "        hdr += add_header\n",
    "\n",
    "    # Set the new name\n",
    "    newname = \"\"\n",
    "    for k in rename_by:\n",
    "        newname += str(hdr[k])\n",
    "        newname += delimiter\n",
    "    newname = newname[:-1] + '.fits'\n",
    "\n",
    "    # Set the new path\n",
    "    newpath = Path(fpath.parent)\n",
    "    if mkdir_by is not None:\n",
    "        for k in mkdir_by:\n",
    "            newpath = newpath / hdr[k]\n",
    "    newpath = newpath / newname\n",
    "    mkdir(newpath.parent)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Rename {fpath.name} to {newpath}\")\n",
    "    fpath.rename(newpath)\n",
    "\n",
    "    return newpath\n",
    "\n",
    "\n",
    "def load_if_exists(path, loader, if_not=None, **kwargs):\n",
    "    ''' Load a file if it exists.\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: pathlib.Path of Path-like str\n",
    "        The path to be searched.\n",
    "    loader: a function\n",
    "        The loader to load ``path``. Can be ``CCDData.read``, ``np.loadtxt``, etc.\n",
    "    if_not: str\n",
    "        Give a python code as a str to be run if the loading failed.\n",
    "    Returns\n",
    "    -------\n",
    "    loaded:\n",
    "        The loaded file. If the file does not exist, ``None`` is returned.\n",
    "    '''\n",
    "    path = Path(path)\n",
    "    if path.exists():\n",
    "        print(f'Loading the existing {str(path)}...', end='')\n",
    "        loaded = loader(path, **kwargs)\n",
    "        print(\" Done\")\n",
    "    elif if_not is not None:\n",
    "        loaded = eval(if_not)\n",
    "    else:\n",
    "        loaded = None\n",
    "    return loaded\n",
    "\n",
    "def stack_FITS(filelist, extension, unit='adu', trim_fits_section=None,\n",
    "               type_key=None, type_val=None):\n",
    "    ''' Stacks the FITS files specified in filelist\n",
    "    Parameters\n",
    "    ----------\n",
    "    filelist: str, path-like, or list of such\n",
    "        The list of FITS files to be stacked\n",
    "\n",
    "    extension: int or str\n",
    "        The extension of FITS to be stacked. For single extension, set it as 0.\n",
    "\n",
    "    unit: Unit or str, optional\n",
    "\n",
    "    trim_fits_section: str, optional\n",
    "        Region of ``ccd`` to be trimmed; see ``ccdproc.subtract_overscan`` for\n",
    "        details. Default is None.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    all_ccd: list\n",
    "        list of ``CCDData``\n",
    "    '''\n",
    "\n",
    "    iskey = False\n",
    "    filelist = list(filelist)\n",
    "\n",
    "    if ((type_key == None) ^ (type_val == None)):\n",
    "        raise KeyError(\n",
    "            \"type_key and type_val must be both specified or both None.\")\n",
    "\n",
    "    if type_key != None:\n",
    "        iskey = True\n",
    "        if isinstance(type_key, str):\n",
    "            type_key = [type_key]\n",
    "        if isinstance(type_val, str):\n",
    "            type_val = [type_val]\n",
    "\n",
    "        if len(type_key) != len(type_val):\n",
    "            raise ValueError(\n",
    "                \"type_key and type_val must be of the same length.\")\n",
    "\n",
    "    all_ccd = []\n",
    "\n",
    "    for i, fname in enumerate(filelist):\n",
    "        ccd_i = CCDData.read(fname, hdu=extension,\n",
    "                             unit=unit)  # unit is just dummy\n",
    "\n",
    "        if iskey:\n",
    "            mismatch = False\n",
    "            for k, v in zip(type_key, type_val):\n",
    "                if (ccd_i.header[k] != v):\n",
    "                    mismatch = True\n",
    "                    break\n",
    "            if mismatch:\n",
    "                continue\n",
    "\n",
    "        if trim_fits_section is not None:\n",
    "            ccd_i = ccdproc.trim_image(ccd_i, fits_section=trim_fits_section)\n",
    "\n",
    "        all_ccd.append(ccd_i)\n",
    "#        im_i = hdu_i[extension].data\n",
    "#        if (i == 0):\n",
    "#            all_data = im_i\n",
    "#        elif (i > 0):\n",
    "#            all_data = np.dstack( (all_data, im_i) )\n",
    "\n",
    "    if len(all_ccd) == 0:\n",
    "        if iskey:\n",
    "            warnings.warn('No FITS file had \"{:s} = {:s}\"'.format(str(type_key),\n",
    "                                                                  str(type_val)))\n",
    "        else:\n",
    "            warnings.warn('No FITS file found')\n",
    "    else:\n",
    "        if iskey:\n",
    "            print('{:d} FITS files with \"{:s} = {:s}\"'\n",
    "                  ' are loaded.'.format(len(all_ccd),\n",
    "                                        str(type_key),\n",
    "                                        str(type_val)))\n",
    "        else:\n",
    "            print('{:d} FITS files are loaded.'.format(len(all_ccd)))\n",
    "\n",
    "    return all_ccd\n",
    "\n",
    "\n",
    "def CCDData_astype(ccd, dtype='float32'):\n",
    "    ccd.data = ccd.data.astype(dtype)\n",
    "    try:\n",
    "        ccd.uncertainty.array = ccd.uncertainty.array.astype(dtype)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    return ccd\n",
    "\n",
    "\n",
    "def combine_ccd(fitslist, trim_fits_section=None, output=None, unit='adu',\n",
    "                subtract_frame=None, combine_method='median', reject_method=None,\n",
    "                normalize=False, exposure_key='EXPTIME',\n",
    "                combine_uncertainty_function=ccdproc_mad2sigma_func,\n",
    "                extension=0, min_value=0, type_key=None, type_val=None,\n",
    "                dtype=np.float32, output_verify='fix', overwrite=False,\n",
    "                **kwargs):\n",
    "    ''' Combining images\n",
    "    Slight variant from ccdproc.\n",
    "    # TODO: accept the input like ``sigma_clip_func='median'``, etc.\n",
    "    # TODO: normalize maybe useless..?\n",
    "    Parameters\n",
    "    ----------\n",
    "    fitslist: list of str, path-like\n",
    "        list of FITS files.\n",
    "\n",
    "    combine: str\n",
    "        The ``method`` for ``ccdproc.combine``, i.e., {'average', 'median', 'sum'}\n",
    "\n",
    "    reject: str\n",
    "        Made for simple use of ``ccdproc.combine``,\n",
    "        {None, 'minmax', 'sigclip' == 'sigma_clip', 'extrema'}. Automatically turns\n",
    "        on the option, e.g., ``clip_extrema = True`` or ``sigma_clip = True``.\n",
    "        Leave it blank for no rejection.\n",
    "\n",
    "    type_key, type_val: str, list of str\n",
    "        The header keyword for the ccd type, and the value you want to match.\n",
    "        For an open HDU named ``hdu``, e.g., only the files which satisfies\n",
    "        ``hdu[extension].header[type_key] == type_val`` among all the ``fitslist``\n",
    "        will be used.\n",
    "\n",
    "    **kwarg:\n",
    "        kwargs for the ``ccdproc.combine``. See its documentation.\n",
    "        This includes (RHS are the default values)\n",
    "        ```\n",
    "        weights=None,\n",
    "        scale=None,\n",
    "        mem_limit=16000000000.0,\n",
    "        clip_extrema=False,\n",
    "        nlow=1,\n",
    "        nhigh=1,\n",
    "        minmax_clip=False,\n",
    "        minmax_clip_min=None,\n",
    "        minmax_clip_max=None,\n",
    "        sigma_clip=False,\n",
    "        sigma_clip_low_thresh=3,\n",
    "        sigma_clip_high_thresh=3,\n",
    "        sigma_clip_func=<numpy.ma.core._frommethod instance>,\n",
    "        sigma_clip_dev_func=<numpy.ma.core._frommethod instance>,\n",
    "        dtype=None,\n",
    "        combine_uncertainty_function=None, **ccdkwargs\n",
    "        ```\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    master: astropy.nddata.CCDData\n",
    "        Resulting combined ccd.\n",
    "\n",
    "    '''\n",
    "\n",
    "    def _set_reject_method(reject_method):\n",
    "        ''' Convenience function for ccdproc.combine reject switches\n",
    "        '''\n",
    "        clip_extrema, minmax_clip, sigma_clip = False, False, False\n",
    "\n",
    "        if reject_method == 'extrema':\n",
    "            clip_extrema = True\n",
    "        elif reject_method == 'minmax':\n",
    "            minmax_clip = True\n",
    "        elif ((reject_method == 'sigma_clip') or (reject_method == 'sigclip')):\n",
    "            sigma_clip = True\n",
    "        else:\n",
    "            if reject_method is not None:\n",
    "                raise KeyError(\"reject must be one of \"\n",
    "                            \"{None, 'minmax', 'sigclip' == 'sigma_clip', 'extrema'}\")\n",
    "\n",
    "        return clip_extrema, minmax_clip, sigma_clip\n",
    "\n",
    "\n",
    "    def _print_info(combine_method, Nccd, reject_method, **kwargs):\n",
    "        if reject_method is None:\n",
    "            reject_method = 'no'\n",
    "\n",
    "        info_str = ('\"{:s}\" combine {:d} images by \"{:s}\" rejection')\n",
    "\n",
    "        print(info_str.format(combine_method, Nccd, reject_method))\n",
    "        print(dict(**kwargs))\n",
    "        return\n",
    "\n",
    "\n",
    "    def _ccdproc_combine(ccdlist, combine_method, min_value=0,\n",
    "                        combine_uncertainty_function=ccdproc_mad2sigma_func,\n",
    "                        **kwargs):\n",
    "        ''' Combine after minimum value correction and then rejection/trimming.\n",
    "        ccdlist:\n",
    "            list of CCDData\n",
    "\n",
    "        combine_method: str\n",
    "            The ``method`` for ``ccdproc.combine``, i.e., {'average', 'median',\n",
    "            'sum'}\n",
    "\n",
    "        **kwargs:\n",
    "            kwargs for the ``ccdproc.combine``. See its documentation.\n",
    "        '''\n",
    "        if not isinstance(ccdlist, list):\n",
    "            ccdlist = [ccdlist]\n",
    "\n",
    "        # copy for safety\n",
    "        use_ccds = ccdlist.copy()\n",
    "\n",
    "        # minimum value correction and trim\n",
    "        for ccd in use_ccds:\n",
    "            ccd.data[ccd.data < min_value] = min_value\n",
    "\n",
    "        #combine\n",
    "        ccd_combined = combine(img_list=use_ccds,\n",
    "                            method=combine_method,\n",
    "                            combine_uncertainty_function=combine_uncertainty_function,\n",
    "                            **kwargs)\n",
    "\n",
    "        return ccd_combined\n",
    "\n",
    "\n",
    "    def _normalize_exptime(ccdlist, exposure_key):\n",
    "        _ccdlist = ccdlist.copy()\n",
    "        exptimes = []\n",
    "\n",
    "        for i in range(len(_ccdlist)):\n",
    "            exptime = _ccdlist[i].header[exposure_key]\n",
    "            exptimes.append(exptime)\n",
    "            _ccdlist[i] = _ccdlist[i].divide(exptime)\n",
    "\n",
    "        if len(np.unique(exptimes)) != 1:\n",
    "            print('There are more than one exposure times:')\n",
    "            print('\\texptimes =', end=' ')\n",
    "            print(np.unique(exptimes), end=' ')\n",
    "            print('seconds')\n",
    "        print('Normalized images by exposure time (\"{:s}\").'.format(exposure_key))\n",
    "\n",
    "        return _ccdlist\n",
    "\n",
    "\n",
    "    fitslist = list(fitslist)\n",
    "\n",
    "    if (output is not None) and (Path(output).exists()):\n",
    "        if overwrite:\n",
    "            print(f\"{output} already exists:\\n\\t\", end='')\n",
    "            print(\"But will be overridden.\")\n",
    "        else:\n",
    "            print(f\"{output} already exists:\\n\\t\", end='')\n",
    "            return load_if_exists(output, loader=CCDData.read, if_not=None)\n",
    "\n",
    "\n",
    "    ccdlist = stack_FITS(filelist = fitslist,\n",
    "                         extension = extension,\n",
    "                         unit = unit,\n",
    "                         trim_fits_section = trim_fits_section,\n",
    "                         type_key = type_key,\n",
    "                         type_val = type_val)\n",
    "    header = ccdlist[0].header\n",
    "\n",
    "    _print_info(combine_method = combine_method,\n",
    "                Nccd = len(ccdlist),\n",
    "                reject_method = reject_method,\n",
    "                min_value = min_value,\n",
    "                dtype = dtype,\n",
    "                **kwargs)\n",
    "\n",
    "    # Normalize by exposure\n",
    "    if normalize:\n",
    "        ccdlist = _normalize_exptime(ccdlist, exposure_key)\n",
    "\n",
    "    # Set rejection switches\n",
    "    clip_extrema, minmax_clip, sigma_clip = _set_reject_method(reject_method)\n",
    "\n",
    "    master = _ccdproc_combine(ccdlist=ccdlist,\n",
    "                              combine_method=combine_method,\n",
    "                              min_value=min_value,\n",
    "                              clip_extrema=clip_extrema,\n",
    "                              minmax_clip=minmax_clip,\n",
    "                              sigma_clip=sigma_clip,\n",
    "                              combine_uncertainty_function=combine_uncertainty_function,\n",
    "                              **kwargs)\n",
    "\n",
    "    str_history = '{:d} images {:s} combined for {:s} = {:s}'\n",
    "    header.add_history(str_history.format(len(ccdlist),\n",
    "                                          str(combine_method),\n",
    "                                          str(type_key),\n",
    "                                          str(type_val)))\n",
    "    header[\"NCOMBINE\"] = len(ccdlist)\n",
    "\n",
    "    if subtract_frame is not None:\n",
    "        subtract = CCDData(subtract_frame.copy())\n",
    "        master = master.subtract(subtract)\n",
    "        header.add_history(\"Subtracted a user-provided frame\")\n",
    "\n",
    "    master.header = header\n",
    "    master = CCDData_astype(master, dtype=dtype)\n",
    "\n",
    "    if output is not None:\n",
    "        master.write(output, output_verify=output_verify, overwrite=overwrite)\n",
    "\n",
    "    return master\n",
    "\n",
    "def make_errmap(ccd, gain_epadu, ronoise_electron=0,\n",
    "                subtracted_dark=None):\n",
    "    ''' Calculate the usual error map.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ccd: array-like\n",
    "        The ccd data which will be used to generate error map. It must be bias\n",
    "        subtracted. If dark is subtracted, give ``subtracted_dark``. If the\n",
    "        amount of this subtracted dark is negligible, you may just set\n",
    "        ``subtracted_dark = None`` (default).\n",
    "    gain: float, array-like, or Quantity\n",
    "        The effective gain factor in ``electron/ADU`` unit.\n",
    "    ronoise: float, array-like, or Quantity, optional.\n",
    "        The readout noise. Put ``ronoise=0`` will calculate only the Poissonian\n",
    "        error. This is useful when generating noise map for dark frames.\n",
    "    subtracted_dark: array-like\n",
    "        The subtracted dark map.\n",
    "    '''\n",
    "    data = ccd.copy()\n",
    "\n",
    "    if isinstance(data, CCDData):\n",
    "        data = data.data\n",
    "\n",
    "    data[data < 0] = 0 # make all negative pixel to 0\n",
    "    \n",
    "    if isinstance(gain_epadu, u.Quantity):\n",
    "        gain_epadu = gain.to(u.electron / u.adu).value\n",
    "\n",
    "    if isinstance(ronoise_electron, u.Quantity):\n",
    "        ronoise_electron = ronoise_electron.to(u.electron)\n",
    "\n",
    "    # Get Poisson noise\n",
    "    if subtracted_dark is not None:\n",
    "        dark = subtracted_dark.copy()\n",
    "        if isinstance(dark, CCDData):\n",
    "            dark = dark.data\n",
    "        # If subtracted dark is negative, this may cause negative pixel in ``data``:\n",
    "        data += dark\n",
    "\n",
    "    var_Poisson = data / gain_epadu  # (data * gain) / gain**2 to make it ADU\n",
    "    var_ROnoise = (ronoise_electron / gain_epadu)**2\n",
    "\n",
    "    errmap = np.sqrt(var_Poisson + var_ROnoise)\n",
    "\n",
    "    return errmap\n",
    "\n",
    "\n",
    "def make_summary(filelist, extension=0, fname_option = 'relative',\n",
    "                 output=None, format='ascii.csv',\n",
    "                 keywords = [], dtypes = [], chmod = 777,\n",
    "                 example_header = None, sort_by='file', verbose=True):\n",
    "    \"\"\" Extracts summary from the headers of FITS files.\n",
    "    Parameters\n",
    "    ----------\n",
    "    filelist: list of str (path-like)\n",
    "        The list of file paths relative to the current working directory.\n",
    "\n",
    "    extension: int or str\n",
    "        The extension to be summarized.\n",
    "\n",
    "    fname_option: str {'absolute', 'relative', 'name'}\n",
    "        Whether to save full absolute/relative path or only the filename.\n",
    "\n",
    "    ouput: str or path-like\n",
    "        The directory and file name of the output summary file. Leave blank\n",
    "        for not saving anything.\n",
    "\n",
    "    format: str\n",
    "        The astropy.table.Table output format.\n",
    "\n",
    "    keywords: list\n",
    "        The list of the keywords to extract (keywords should be in ``str``).\n",
    "\n",
    "    dtypes: list\n",
    "        The list of dtypes of keywords if you want to specify. If ``[]``,\n",
    "        ``['U80'] * len(keywords)`` will be used. Otherwise, it should have\n",
    "        the same length with ``keywords``.\n",
    "\n",
    "    chmod: int\n",
    "        the chmod code (e.g., 777 for ``rwxrwxrwx``).\n",
    "\n",
    "    example_header: str or path-like\n",
    "        The path including the filename of the output summary text file.\n",
    "\n",
    "    sort_by: str\n",
    "        The column name to sort the results. It can be any element of\n",
    "        ``keywords`` or ``'file'``, which sorts the table by the file name.\n",
    "    \"\"\"\n",
    "\n",
    "    def _get_fname(path):\n",
    "        if fname_option == 'relative':\n",
    "            return str(path)\n",
    "        elif fname_option == 'absolute':\n",
    "            return str(path.absolute())\n",
    "        else:\n",
    "            return path.name\n",
    "\n",
    "    options = ['absolute', 'relative', 'name']\n",
    "    if fname_option not in options:\n",
    "        raise KeyError(f\"fname_option must be one of {options}.\")\n",
    "\n",
    "    skip_keys = ['COMMENT', 'HISTORY']\n",
    "\n",
    "    if verbose:\n",
    "        if (keywords != []) and (keywords != '*'):\n",
    "            print(\"Extracting keys: \", keywords)\n",
    "        str_example_hdr = \"Extract example header from {:s}\\n\\tand save as {:s}\"\n",
    "        str_keywords = \"All {:d} keywords will be loaded.\"\n",
    "        str_keyerror_fill = \"Key {:s} not found for {:s}, filling with '--'.\"\n",
    "        str_valerror = \"Please use 'U80' as the dtype for the key {:s}.\"\n",
    "        str_filesave = 'Saving the summary file to \"{:s}\"'\n",
    "\n",
    "\n",
    "\n",
    "    # Save example header\n",
    "    if example_header is not None:\n",
    "        example_fits = filelist[0]\n",
    "        if verbose:\n",
    "            print(str_example_hdr.format(str(example_fits), example_header))\n",
    "        ex_hdu = fits.open(example_fits)\n",
    "        ex_hdr = ex_hdu[extension].header\n",
    "        ex_hdr.totextfile(example_header, overwrite=True)\n",
    "\n",
    "    # load ALL keywords for special cases\n",
    "    if (keywords == []) or (keywords == '*'):\n",
    "        example_fits = filelist[0]\n",
    "        ex_hdu = fits.open(example_fits)\n",
    "        ex_hdu.verify('fix')\n",
    "        ex_hdr = ex_hdu[extension].header\n",
    "        N_hdr = len(ex_hdr.cards)\n",
    "        keywords = []\n",
    "        for i in range(N_hdr):\n",
    "            key_i = ex_hdr.cards[i][0]\n",
    "            if (key_i in skip_keys):\n",
    "                continue\n",
    "            elif (key_i in keywords):\n",
    "                str_duplicate = \"Key {:s} is duplicated! Only first one will be saved.\"\n",
    "                print(str_duplicate.format(key_i))\n",
    "                continue\n",
    "            keywords.append(key_i)\n",
    "        if verbose:\n",
    "            print(str_keywords.format(len(keywords)))\n",
    "\n",
    "    # Initialize\n",
    "    if len(dtypes) == 0:\n",
    "        dtypes = ['U80'] * len(keywords)\n",
    "        # FITS header MUST be within 80 characters! (FITS standard)\n",
    "\n",
    "    summarytab = Table(names=keywords, dtype=dtypes)\n",
    "    fnames = []\n",
    "\n",
    "    # Run through all the fits files\n",
    "    for fitsfile in filelist:\n",
    "        fitsfile.chmod(chmod)\n",
    "        fnames.append(_get_fname(fitsfile))\n",
    "        hdu = fits.open(fitsfile)\n",
    "        hdu.verify('fix')\n",
    "        hdr = hdu[extension].header\n",
    "        row = []\n",
    "        for key in keywords:\n",
    "            try:\n",
    "                row.append(hdr[key])\n",
    "            except KeyError:\n",
    "                if verbose:\n",
    "                    print(str_keyerror_fill.format(key, str(fitsfile)))\n",
    "                try:\n",
    "                    row.append('--')\n",
    "                except ValueError:\n",
    "                    raise ValueError(str_valerror.format('U80'))\n",
    "        summarytab.add_row(row)\n",
    "        hdu.close()\n",
    "\n",
    "    # Attache the file name, and then sort by file name.\n",
    "    fnames = Column(data=fnames, name='file')\n",
    "    summarytab.add_column(fnames, index=0)\n",
    "    summarytab.sort(sort_by)\n",
    "\n",
    "\n",
    "    # sort by a key if ``sort_by`` is given\n",
    "    if ((sort_by != '') and (sort_by != None)):\n",
    "        summarytab.sort('file')\n",
    "\n",
    "    if output is not None:\n",
    "        if verbose:\n",
    "            print(str_filesave.format(str(output)))\n",
    "        summarytab.write(output, format=format, overwrite=True)\n",
    "\n",
    "    return summarytab\n",
    "\n",
    "    \n",
    "\n",
    "def Gfit2hist(data):\n",
    "    ''' Gaussian fit to the frequency distribution of the nddata.\n",
    "    '''\n",
    "    freq = itemfreq(data.flatten())\n",
    "    fitter = LevMarLSQFitter()\n",
    "    mode = freq[freq[:, 1] == freq[:, 1].max(), 0][0]\n",
    "    init = Gaussian1D(mean=mode)\n",
    "    fitG = fitter(init, freq[:, 0], freq[:, 1])\n",
    "    return fitG\n",
    "\n",
    "def bias2ronoise(data):\n",
    "    ''' Infer readout noise from bias image.\n",
    "    '''\n",
    "    fitG = Gfit2hist(data)\n",
    "    return fitG.stddev.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above functions are a part of my personal package for astronomical data reduction and simulation. If you want, you may copy-and-paste the above code and save it as, e.g., ``AO1util.py``, and at the same directory, you can import that file by using\n",
    "```python\n",
    "import AO1util\n",
    "AO1util.make_summary(~~~~~~)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. File Renamer\n",
    "\n",
    "TRIPOL by default takes 3-band images (g' r' i') simultaneously and saves them with file name ``f_yymmdd_xxxx.fits`` where ``f`` is the filter name (``g``, ``r``, ``i``), ``yymmdd`` is the sidereal date (e.g., ``180412``), and ``xxxx`` is the counter with up to 4 digits with leading zero filling (e.g., ``0012``).\n",
    "\n",
    "There are some inconveniences on this:\n",
    "\n",
    "1. Object name and exposure times are not written.\n",
    "2. The three FITS files (g, r, i bands) share the same counter.\n",
    "3. Flat images must be taken differently for each band but all 3 images are saved. You must specify which flat you took in ``OBJECT`` key in the header.\n",
    "4. Since bias and dark images are different in each CCD, we need to make 3 master dark or bias images, too.\n",
    "\n",
    "So I want to first rename all the files:\n",
    "\n",
    "1. Make name as ``counter_target_exposuretime.fits``.\n",
    "2. Save that file at a directory ``g``, ``r``, or ``i`` depending on their header keyword ``FILTER``.\n",
    "\n",
    "Then you may do preprocessing at each directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting TL image: i.fits\n"
     ]
    }
   ],
   "source": [
    "rename_by = [\"COUNTER\", \"OBJECT\", \"EXPOS\"]\n",
    "mkdir_by = [\"FILTER\"]\n",
    "TOPPATH = Path('180412', 'rawdata')\n",
    "REDUCED = Path('180412', 'reduced')\n",
    "mkdir(REDUCED)\n",
    "# Set the path as the path where your FITS files are\n",
    "filelist = list(TOPPATH.glob('*.fits'))\n",
    "\n",
    "for fpath in filelist:\n",
    "    # If it is TL image (e.g., ``g.fits``), delete it\n",
    "    try:\n",
    "        counter = fpath.name.split('_')[1][:4]\n",
    "    except IndexError:\n",
    "        print(f\"deleting TL image: {fpath.name}\")\n",
    "        fpath.unlink()\n",
    "        continue\n",
    "    \n",
    "    # Set the ``COUNTER`` keyword \n",
    "    counter = fpath.name.split('_')[1][:4]\n",
    "    newname = fitsrenamer(fpath, rename_by=rename_by, delimiter='_', \n",
    "                          add_header={\"COUNTER\": counter},\n",
    "                          mkdir_by=mkdir_by,\n",
    "                          verbose=False)\n",
    "print(\"Renaming done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting files will have this kind of tree:\n",
    "```\n",
    "├── rawdata\n",
    "│   ├── g\n",
    "│   │   ├── 0001_TEST_1.0.fits\n",
    "│   │   ├── 0002_TEST_1.0.fits\n",
    "│   │   ├── 0003_TEST_1.0.fits\n",
    "...\n",
    "│   ├── i\n",
    "│   │   ├── 0001_TEST_1.0.fits\n",
    "│   │   ├── 0002_TEST_1.0.fits\n",
    "│   │   ├── 0003_TEST_1.0.fits\n",
    "...\n",
    "│   └── r\n",
    "│       ├── 0001_TEST_1.0.fits\n",
    "│       ├── 0002_TEST_1.0.fits\n",
    "│       ├── 0003_TEST_1.0.fits\n",
    "...\n",
    "3 directories, 429 files\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Bias, Dark, and Flat\n",
    "\n",
    "In this code, I will make bias for each filter, and dark for each filter and each exposure time. Then make bias and dark subtracted flat image.\n",
    "\n",
    "The files ``Dark_<filter>_<exptime>.fits`` are the raw darks (bias not subtracted). \n",
    "\n",
    "For each CCD, we have dummy flats, so we have to carefully choose which frame to use for each CCD's flat. In the observation, we set the ``OBJECT`` key value as ``flat_x`` for filter ``x`` (``g``, ``r``, ``i``). For flat, bias and puredark (bias subtracted dark) are both subtracted, and there will be **one more extension called ``\"UNCERT\"``.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccdproc import subtract_bias, subtract_dark, flat_correct, create_deviation\n",
    "\n",
    "# Initial setting\n",
    "MEDCOMB_KEYS = dict(dtype='float32',\n",
    "                    combine_method=\"median\",\n",
    "                    reject_method=None,\n",
    "                    combine_uncertainty_function=None)\n",
    "keymap = dict(EXPTIME = 'EXPOS', GAIN = 'EGAIN', OBJECT = 'OBJECT', FILTER='FILTER')\n",
    "filters = ['g', 'r', 'i']\n",
    "gains = {}\n",
    "\n",
    "# Do dark combine, etc, for each filter:\n",
    "for filt in filters:\n",
    "    # Find all FITS files and make a summary table\n",
    "    filelist = list((TOPPATH / filt).glob('*.fits'))\n",
    "    filetable = make_summary(filelist, output=None)\n",
    "    # If you want to save and manually see the summary. See the homework below.\n",
    "    \n",
    "    # Select only \"dark\" frames and \"flat\" of given filter\n",
    "    darktable = filetable[filetable[keymap[\"OBJECT\"]]==\"dark\"]\n",
    "    flattable = filetable[filetable[keymap[\"OBJECT\"]]==f\"flat_{filt}\"]\n",
    "\n",
    "    # Set save path\n",
    "    biaspath = REDUCED / f\"Bias_{filt}.fits\"\n",
    "    flatpath = REDUCED / f\"Flat_{filt}.fits\"\n",
    "\n",
    "\n",
    "    # Initialize for different exptime for dark frames\n",
    "    darks = {}\n",
    "    \n",
    "    # For each dark frames grouped by exposure time, combine and save dark frame:\n",
    "    for group in darktable.group_by(keymap[\"EXPTIME\"]).groups:\n",
    "        # Find exposure time\n",
    "        exptime_dark = float(group[keymap[\"EXPTIME\"]][0])\n",
    "        \n",
    "        # Set save path\n",
    "        darkpath = REDUCED / f\"Dark_{filt}_{exptime_dark}s.fits\"\n",
    "        \n",
    "        # Dark combine\n",
    "        mdark = combine_ccd(group[\"file\"], \n",
    "                            output=darkpath,\n",
    "                            overwrite=True,\n",
    "                            **MEDCOMB_KEYS,\n",
    "                            type_key=[keymap[\"FILTER\"], keymap[\"EXPTIME\"]],\n",
    "                            type_val=[filt, exptime_dark])\n",
    "        \n",
    "        # archive for future use (if memory is problematic, you shouldn't do this)\n",
    "        darks[exptime_dark] = mdark\n",
    "\n",
    "    # Find suitable bias image\n",
    "    exptime_dark_min = min(darks.keys())\n",
    "    if exptime_dark_min < 1:\n",
    "        # Find which to use bias\n",
    "        bias = darks[exptime_dark_min]\n",
    "        bias.write(biaspath, overwrite=True)\n",
    "        print(f\"Using dark with exposure {min(darks.keys())}s as bias ({biaspath})\")\n",
    "    \n",
    "    else:\n",
    "        warnings.warn(f\"Minimum dark exposure is too long ({exptime_dark_min} sec)!\"\n",
    "                      \" I think you have no file for bias...\")\n",
    "    \n",
    "    gain = float(darktable[keymap[\"GAIN\"]][0]) * u.electron / u.adu\n",
    "    gains[filt] = gain\n",
    "    \n",
    "    # Flat combine \n",
    "    exptime_flat = float(flattable[keymap['EXPTIME']][0])\n",
    "    mflat = combine_ccd(flattable[\"file\"], \n",
    "                        **MEDCOMB_KEYS,\n",
    "                        type_key=[keymap[\"FILTER\"], keymap[\"EXPTIME\"]],\n",
    "                        type_val=[filt, exptime_flat])\n",
    "    \n",
    "    # Make error map for flat (Poisson only)\n",
    "    mflat_err = np.sqrt((mflat.data - bias.data) / gain.value)\n",
    "    mflat.uncertainty = mflat_err\n",
    "    \n",
    "    # Set appropriate dark (which includes bias)\n",
    "    try:\n",
    "        dark = darks[exptime_flat].data\n",
    "        \n",
    "    except KeyError:\n",
    "        # If no suitable exptime in darks, generate dark using linear scaling\n",
    "        exptime_dark_max = max(darks.keys())\n",
    "        darkmax = darks[exptime_dark_max]\n",
    "        puredark = (darkmax.data - bias.data) * (exptime_flat/exptime_dark_max)\n",
    "        dark = puredark + bias.data\n",
    "    \n",
    "    # Do dark subtraction\n",
    "    mflat.data = mflat.data - dark\n",
    "    mflat = CCDData_astype(mflat, dtype='float32')\n",
    "    mflat.write(flatpath, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now have this kind of tree:\n",
    "```\n",
    "├── rawdata\n",
    "│   ├── g\n",
    "│   │   ├── 0001_TEST_1.0.fits\n",
    "│   │   ├── 0002_TEST_1.0.fits\n",
    "│   │   ├── 0003_TEST_1.0.fits\n",
    "...\n",
    "│   ├── i\n",
    "│   │   ├── 0001_TEST_1.0.fits\n",
    "│   │   ├── 0002_TEST_1.0.fits\n",
    "│   │   ├── 0003_TEST_1.0.fits\n",
    "...\n",
    "│   └── r\n",
    "│       ├── 0001_TEST_1.0.fits\n",
    "│       ├── 0002_TEST_1.0.fits\n",
    "│       ├── 0003_TEST_1.0.fits\n",
    "...\n",
    "└── reduced\n",
    "    ├── Bias_g.fits\n",
    "    ├── Bias_i.fits\n",
    "    ├── Bias_r.fits\n",
    "    ├── Dark_g_0.4s.fits\n",
    "    ├── Dark_g_120.0s.fits\n",
    "    ├── Dark_g_2.0s.fits\n",
    "    ├── Dark_g_5.0s.fits\n",
    "    ├── Dark_i_0.4s.fits\n",
    "    ├── Dark_i_120.0s.fits\n",
    "    ├── Dark_i_2.0s.fits\n",
    "    ├── Dark_i_5.0s.fits\n",
    "    ├── Dark_r_0.4s.fits\n",
    "    ├── Dark_r_120.0s.fits\n",
    "    ├── Dark_r_2.0s.fits\n",
    "    ├── Dark_r_5.0s.fits\n",
    "    ├── Flat_g.fits\n",
    "    ├── Flat_i.fits\n",
    "    └── Flat_r.fits\n",
    "\n",
    "5 directories, 448 files\n",
    "\n",
    "```\n",
    "* **HOMEWORK**: Check the file sizes of bias, dark, and flat images. Why do you think the size of flat images are twice the size of the bias and dark images?\n",
    "* **HOMEWORK**: Think about the validity of the code of ``mflat_err`` and the code in ``except KeyError:`` block when calculating pure dark. Think why the calculation is done in that way.\n",
    "* **HOMEWORK**: Try the following code and open the resulting file.\n",
    "\n",
    "```python\n",
    "filelist = list(Path('.', '180412').glob(\"**/*.fits\"))\n",
    "filetable = make_summary(filelist, output=\"test.csv\", format=\"ascii.csv\", \n",
    "                         keywords=[\"NAXIS1\", \"NAXIS2\", \"DATE\", \"OBJECT\", \"EXPOS\", \n",
    "                                   \"FILTER\", \"XBIN\", \"YBIN\", \"CCD_TEMP\", \"CCD_COOL\", \"EGAIN\"])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Object Frame\n",
    "Now that we have all the preprocessing frames! (bias, dark, flat).\n",
    "\n",
    "One thing we don't have in the header is the readout noise. It can roughly be calculated by fitting a Gaussian curve to the frequency distribution of bias frame (the standard deviation of the fitting curve is the readout noise in ADU). Since we know the electron gain from the header, although we are not sure about this value, you can convert the readout noise into electron unit, if you wish.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccdproc import ccd_process \n",
    "from astropy.nddata import StdDevUncertainty\n",
    "\n",
    "objnames = [\"HIP61602\", \"HIP66872\"]\n",
    "\n",
    "for filt in filters:\n",
    "    # Find all FITS files and make a summary table\n",
    "    filelist = list((TOPPATH / filt).glob('*.fits'))\n",
    "    filetable = make_summary(filelist, output=None)\n",
    "    gain = gains[filt]\n",
    "    # Group by the object name and then exposure time\n",
    "    for group in filetable.group_by([keymap[\"OBJECT\"], keymap[\"EXPTIME\"]]).groups:\n",
    "        # If not the name of object of interest, continue\n",
    "        objname = group[keymap[\"OBJECT\"]][0]\n",
    "        if objname not in objnames:\n",
    "            continue\n",
    "        # Find the exposure time\n",
    "        exptime_obj = group[keymap[\"EXPTIME\"]][0]\n",
    "        \n",
    "        # Set paths and read preprocessing frames\n",
    "        biaspath = REDUCED / f\"Bias_{filt}.fits\"\n",
    "        darkpath = REDUCED / f\"Dark_{filt}_{exptime_obj}s.fits\"\n",
    "        flatpath = REDUCED / f\"Flat_{filt}.fits\"\n",
    "        mbias = CCDData.read(biaspath)\n",
    "        mdark = CCDData.read(darkpath)\n",
    "        mflat = CCDData.read(flatpath)\n",
    "        \n",
    "        # Make \"pure dark\" by bias subtraction for generating error map.\n",
    "        pdark = subtract_bias(mdark, mbias)\n",
    "        pdark.uncertainty = StdDevUncertainty(make_errmap(pdark.data, gain.value))\n",
    "        \n",
    "        # Roughly calculate ronoise\n",
    "        ronoise = bias2ronoise(mbias.data) * u.adu * gain\n",
    "        \n",
    "        # For each file, do the preprocessing (it should be straightforward)\n",
    "        for fname in group[\"file\"]:\n",
    "            objpath = REDUCED / f\"{filt}_{Path(fname).name}\"\n",
    "            objccd = CCDData.read(fname, unit='adu')\n",
    "            objccd_b = subtract_bias(objccd, mbias,\n",
    "                                     add_keyword=dict(HISTORY = f\"Bias subtracted using {biaspath}\"))\n",
    "            \n",
    "            # Make errormap AFTER bias subtraction!!\n",
    "            objccd_b.uncertainty = StdDevUncertainty(make_errmap(objccd_b, gain.value, ronoise.value))\n",
    "            objccd_bd = subtract_dark(objccd_b, pdark, \n",
    "                                      exposure_time=keymap[\"EXPTIME\"], \n",
    "                                      exposure_unit=u.s,\n",
    "                                      add_keyword=dict(HISTORY = f\"Dark subtracted using {darkpath}\"))\n",
    "            objccd_bdf = flat_correct(objccd_bd, mflat,\n",
    "                                      add_keyword=dict(HISTORY = f\"Flat corrected using {flatpath}\"))\n",
    "            objccd_bdf = CCDData_astype(objccd_bdf, dtype='float32')\n",
    "            objccd_bdf.write(objpath, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have preprocessed data. Just as an example, I will open a random image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from astropy.visualization import ZScaleInterval, ImageNormalize\n",
    "\n",
    "def znorm(image):\n",
    "    return ImageNormalize(image, interval=ZScaleInterval())\n",
    "\n",
    "\n",
    "def zimshow(ax, image, **kwargs):\n",
    "    return ax.imshow(image, norm=znorm(image), origin='lower', **kwargs)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "zimshow(ax, objccd_bdf.data)\n",
    "ax.set_title(objpath)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not sure what those bright pattern at the lower-left and middle-right (i'-band CCD only). The gradient is not negligible (~ 10% fractional ratio). We are suspecting the infrared CCTV installed inside the dome. \n",
    "\n",
    "Reasons: It appears only in i'-band, so the light source should be red or near IR. The pattern does appear in the raw images too, so it shouldn't be due to flat. Since the pattern does not appear on ~ 3 min exposure dark image, maybe it is not significant when the shutter is closed. The pattern slightly differ for our two targets, so maybe this is direction dependent. So I can suspect stray light from observing room/dome or school buildings nearby the observatory.\n",
    "\n",
    "The ``ginga`` cut images are attached:\n",
    "\n",
    "![](images/ginga_tripol.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
